---
title: "b_2_cleaning_and_processing"
author: "Roi & Mike"
date: "08/02/2023"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyverse)
```

## Data Cleaning and Processing

## Cleaning VMS data

Data cleaning is an extremely important part of the processing of VMS data. Many errors within the data are commonly caused by technical issues. Sometimes VMS points are recorded at longitudes/latitudes that are not on the earth. 

The following code removes VMS points which are recorded incorrectly and so their numbers make it impossible for them to be on the earth. Ideally, the length of 'oog' (number of impossible points) will be low, meaning as many points as possible are usable. These points are identified by their latitudes and longitudes being out of the range of that which we use for the planet: if they fall outside of this range, they must be incorrect and therefore removed.

```{r off_globe, message=FALSE, warning=FALSE}
oog <- which(abs(tacsat$SI_LATI) > 90 | abs(tacsat$SI_LONG) > 180)

oog <- unique(c(oog,which(tacsat$SI_HE < 0 | tacsat$SI_HE >  360))) 
#adding points with heading outside compass range
length(oog)
```

Sometimes duplicates are created, meaning 2 VMS pings are recorded by the same vessel at the exact same time, making an exact duplicate record. The following code creates a column containing the VE_REF, coordinates and the date/time of each VMS ping, in order to establish duplicates.S

```{r duplicates, message=FALSE, warning=FALSE}
tacsat$SI_DATIM <- as.POSIXct(paste(tacsat$SI_DATE,tacsat$SI_TIME,sep=" "), 
                              tz="GMT", format="%d/%m/%Y  %H:%M")
# create a proper date-time stamp column by 'pasting' together VE_REF, the location and the date/time columns. This makes it straightforward to check for duplicates.
uniqueTacsat    <- paste(tacsat$VE_REF,tacsat$SI_LATI,tacsat$SI_LONG,tacsat$SI_DATIM) 
uniqueTacsat[1:5]

print(nrow(tacsat)) # how many rows before duplicates removal?

tacsat          <- tacsat[!duplicated(uniqueTacsat),] # removes duplicated tacsat entries
print(nrow(tacsat)) # how many rows after duplicates removal?
```

Next, points which appear on land must be eliminated. Similarly to not being within the bounds of the earth, points which are registered on land masses cannot be correct and therefore must be removed. This is done by importing a dataset for the European coastline and finding the points which intersect with the land. This is done using the pointOnLand function included with the 

```{r on land, message=FALSE, warning=FALSE}
data(europa) # the dataset for the European coastline comes included in the vmstools package, it is called upon here
class(europa)

idx     <- pointOnLand(tacsat,europa);
table(idx)

pol     <- tacsat[which(idx==1),] #points on land

#Overwrite the original tacsat dataset with only those points not on land
tacsat  <- tacsat[which(idx==0),] #tacsat not on land
```

The errors found up to now are certain; VMS pings should not be duplicated and they cannot come from inland areas. However, some errors are not so obvious. These may include records that are traveling at very high speeds or spend significant time in harbours, rather than the open water. To remove these, a speed threshold can be set, which will eliminate any points with associated speeds higher than the specific value chosen. It is rare for points to have speeds greater than 20 knots, so spThres is set to 20, but this can be changed accordingly.

By creating a summary of the speed column in the tacsat data, it can be seen that the average speeds are around 2-4 knots, whilst the maximum is much higher at 46.81, suggesting it is an outlier. This is supported by the subsequent histogram, which allows for an appropriate threshold to be set based on the frequency of vessels traveling at each speed.

```{r high speed, message=FALSE, warning=FALSE}
summary(tacsat$SI_SP) # summarise the speed (SI_SP) statistics
hist(tacsat$SI_SP,breaks=100)

spThres <- 20 # setting the speed threshold to 20 so all record with speeds greater than 20 are removed
idx <- which(tacsat$SI_SP > spThres) # creating an object containing all records with speed above 20
tacsat <- tacsat[-idx,] # removing the idx object just created, so only the records with <20 speed remain
```

Now to remove points within ports and harbours. This is done by calling on the harbours dataset built into the VMStools package.

```{r harbours, message=FALSE, warning=FALSE}
data(harbours) # calls in the harbours dataset
idx <- pointInHarbour(tacsat$SI_LONG,tacsat$SI_LATI,harbours,saveHarbourList=F) # uses the pointInHarbur function to cross reference the longitudes/latitudes of the tacsat data and mark those within the harbours with a 1

pih <- tacsat[which(idx==1),] # pulls the tacsat data with a 1 associated
table(idx)

#Overwrite the tacsat file with only those records which are not in harbour
tacsat <- tacsat[which(idx==0),] 
```


### Cleaning Logbook data

Errors in logbook data mostly stem from human error, for example an arrival date set before the departure date - an impossible occurrence. Additionally, vessel or gear information such as mesh size or vessel length should be within reasonable bounds; those outside can be removed.

First, check for duplicates.

```{r efl duplicates, message=FALSE, warning=FALSE}
eflalo <- eflalo[!duplicated(eflalo$LE_ID),]
```

If for some reason the data is problematic and the above doesn't work as it should, the following will take many of the columns and combine them into one, creating a unique column due to the number of elements included. Then duplicate rows can be removed.

```{r efl duplicates, message=FALSE, warning=FALSE}

# combine columns into one super column
ID <- paste(eflalo$VE_REF, eflalo$VE_FLT, eflalo$VE_COU, eflalo$VE_LEN, eflalo$VE_KW, eflalo$VE_TON, eflalo$FT_REF, eflalo$FT_DCOU, eflalo$FT_DHAR, eflalo$FT_DDAT, eflalo$FT_DTIME, eflalo$FT_LCOU, eflalo$FT_LHAR, eflalo$FT_LDAT, eflalo$FT_LTIME,eflalo$LE_GEAR,eflalo$LE_RECT,eflalo$LE_MSZ)

# check for and remove duplicates
eflalo <- eflalo[!duplicated(ID),]
```

As previously stated, entries where the landing time/date is earlier than the departure time are clearly wrong, these an be removed by creating a time-stamp for departure and arrival dates and finding entries where landing time is greater than departure.

```{r dates, message=FALSE, warning=FALSE}
eflalop           <- eflalo

#Create a date-time stamp for the departure date
eflalop$FT_DDATIM <- as.POSIXct(paste(eflalo$FT_DDAT,  eflalo$FT_DTIME,   sep=" "), tz="GMT", format="%d/%m/%Y  %H:%M")

#Create a date-time stamp for the landing date
eflalop$FT_LDATIM <- as.POSIXct(paste(eflalo$FT_LDAT,  eflalo$FT_LTIME,   sep=" "), tz="GMT", format="%d/%m/%Y  %H:%M")

#Now see where the landing date happens before departure
idx               <- which(eflalop$FT_LDATIM >= eflalop$FT_DDATIM)
print(nrow(eflalo))

#only keep the records we want
eflalo            <- eflalo[idx,]
print(nrow(eflalo))
```

Extremely large catches must also be dealt with; if the largest catch of a species is over 10x larger than the 2nd largest, it is unlikely to be correct. The following code creates a threshold for each species based on a specified multiplier of the 2nd largest catch per species.

```{r large catch, message=FALSE, warning=FALSE}
# Retrieving the unique names of all species in the datset
specs  <- substr(colnames(eflalo[grep("KG",colnames(eflalo))]),7,9)
lanThres <- 1.5 # this is approximately 30x difference on a log10 scale) This will act as the multiplier to set the individual species thresholds

# Define per species what the maximum allowed catch is (larger than that value you expect it to be an error / outlier
specBounds      <- lapply(as.list(specs),function(x){
idx   <- grep(x,colnames(eflalo))[grep("KG",colnames(eflalo)[grep(x,colnames(eflalo))])];
wgh   <- sort(unique(eflalo[which(eflalo[,idx]>0),idx]));
difw  <- diff(log10(wgh));
return(ifelse(any(difw > lanThres),wgh[rev(which(difw <= lanThres)+1)],ifelse(length(wgh)==0,0,max(wgh,na.rm=T))))})

# Make a list of the species names and their thresholds
specBounds      <- cbind(specs,unlist(specBounds));

# Set NA values to zero to avoid errors
specBounds[which(is.na(specBounds[,2])==T),2] <- "0"

# Get the index (column number) of each of the species
idx             <- unlist(lapply(as.list(specs),function(x){
idx   <- grep(x,colnames(eflalo))               [grep("KG",colnames(eflalo)[grep(x,colnames(eflalo))])];
return(idx)}))

# If landing > threshold then set value to NA
for(iSpec in idx) eflalo[which(eflalo[,iSpec] > an(specBounds[(iSpec-idx[1]+1),2])),iSpec] <- NA

# Following this, set all NA values to 0
for(i in kgeur(colnames(eflalo))) eflalo[which(is.na(eflalo[,i]) == T),i] <- 0
```

Following this cleaning, you can save the outputs of both datasets. They are saved with the suffix Clean, as this is what the next part of the VMStools sequence is expecting. The following code will make an output directory in the current workspace. This location can be found by using the getwd() function.

```{r save, message=FALSE, warning=FALSE}
getwd()

dir.create("./output")
save(tacsat,file="./output/tacsatClean.RData")
save(eflalo,file="./output/eflaloClean.RData")
```




